En este capítulo se entra en detalle sobre la implementación de los distintos componentes del sistema asi como las dificultades encontradas, la matemática detrás de las transformaciones y la complejidad computacional del algoritmo. Finalmente, se reflexiona sobre las decisiones tomadas y la correctitud de las mismas.

\section{Conversor de vídeos (media\_conversor)}

En primer lugar, se debe implementar el programa que convertirá los videos del formato propietario de insta360 a un formato público manteniendo las propiedades esféricas del vídeo.

Antes de desarrollar, se debe de tener acceso al SDK de insta360, al cual se aplica con razones acádemicas en este caso. El proceso de aprobación se extendió una semana y una vez obtenido, la implementación del conversor de videos fue muy sencilla pues el ejemplo de uso propuesto por la compañia es muy similar a la funcionalidad requerida para el proyecto. No se puede entrar muy en detalle de la implementación pues la mayor parte del trabajo la realiza la clase \verb|VideoStitcher| del SDK privado de insta360. Sin embargo, cabe mencionar que el programa viene con soporte de optimizaciones de GPU y paralelización de hilos.

El único desarrollo destacable es el proceso de elección de videos dínamico por el usuario pues a traves de una aplicación de CLI puesto que al tener dos lentes y un video asociado a cada lente, es poco práctico tener que escribir manualmente los \textit{paths} de entrada y salida del programa. A través del uso de Regex.

\CPPCode[picking_files_cpp]{File picking}{Automatización de I/O según la convención de insta360}{main.cc}{52}{86}{}

\vspace{60px}

\section{Rastreador del portero (goalkeeper\_tracker)}

Una vez obtenido el video en un formato con el que se puede trabajar con la librería de openCV, se procede a la implementación del principal objetivo del trabajo, la automatización de edición de jugadas de portero de futbol.

Los pasos a seguir por el programa, independientemente de su implementación son los siguientes.
\begin{enumerate}
	\item Elegir el vídeo de input en formato equirectangular y ya convertido a .mp4
	\item Iterar por cada frame que compone el video
	\item Convertir el frame equirectangular en su proyección de cubemap (6 perspectivas)
	\item Pasar cada una de las 6 perspectivas por el modelo de detección de objetos y  guardar los resultados de interes en una lista
	\item Usar la información de la lista para determinar la nueva perspectiva con la información relevante
	\item Crear la nueva perspectiva y escribirla en el vídeo resultante

\end{enumerate}

\subsection{Conceptos previos}
La proyección de una imagen esférica sobre una superficie plana se conoce como su imágen equirectangular. El trabajo con imágenes esféricas y equirectangulares introduce una serie de dificultades frente al de imagenes planas. Antes de detallar la implementación, hace falta explicar una serie de conceptos asociados a las coordenadas de imágenes esféricas, equirectangulares y cartesianas y la conversión entre las mismas. En la figura \ref{equirectCoordsSpherical} se aprecia una representación semántica de las coordenadas equirectangulares en una imagen omnidireccional. Las coordenadas que indexan una imágen equirectangular vienen dadas por las letras $(\theta, \phi)$.

\begin{figure}[equirectCoordsSpherical]{equirectCoordsSpherical}{Representación de coordenadas equirectangulares en una figura esférica}
	\begin{center}
		\image{180px}{}{assets/omnidirectionalImage}
	\end{center}
\end{figure}

\begin{description}
	\item[$\theta$] Indexa el eje horizontal y toma valores entre $[-\pi, \pi]$ (360 grados). Puede pensarse como la dirección horizontal en la que está mirando la cámara. Viene dado en radianes y en caso de estar dado en grados se conoce como \textit{yaw}
	\item[$\phi$] Indexa el eje vertical y toma valores entre $[-\frac{\pi}{2}, \frac{\pi}{2}]$ (180 grados). Puede pensarse como de arriba o abajo esta apuntando la cámara. Viene dado en radianes y en caso de estar dado en grados se como como \textit{pitch}
\end{description}

Una vez proyectada la imágen esférica sobre una superficie plana se obtiene una imagen como la mostrada en la figura \ref{equirectCoordsPlain}, donde se ve claramente la forma en la que se indexa a través de $\theta, \phi$ (o yaw y pitch en grados) la imagen.


\begin{figure}[equirectCoordsPlain]{equirectCoordsPlain}{Representación de coordenadas equirectangulares en una sueprficie plana}
	\begin{center}
		\image{300px}{}{assets/equirectCoordinates}
	\end{center}
\end{figure}

De esta forma, resulta muy sencillo conceptualmente 'desplegar' la proyección equirectangular en una vista conocida como CubeMap, la cual representa las 6 vistas principales posibles en un espacio tridimensional. Se puede pensar como envolver la vista esférica en un cubo donde cada cara tendrá una perspectiva plana sin información duplicada del resto. La figura \ref{cubemapConcept} muestra una representación gráfica de este concepto.

\begin{figure}[cubemapConcept]{cubemapConcept}{Representación visual de la vista Cubemap a partir de una imagen esférica}
	\begin{center}
		\image{300px}{}{assets/cubemapConcept}
	\end{center}
\end{figure}

\subsection{Transformación de imágenes equirectangulares a planas}
Como ya se ha discutido previamente, el modelo de visión computacional empleado está entrenado con imágenes sin distorsión por lo que ofrecerá mejores resultados si las imagenes procesadas son perspectivas generadas a partir de la imagen equirectangular.

Para crear una imagen sin distorsión a partir de una imagen equirectangular, se requiere de una coordenada que indexe el punto central de la perspectiva y el tamaño del resultado deseado. 
El proceso de transformación de imágenes es costoso y complejo, y su fundamento matemático se encuentra en la trigonometría avanzada y las transformaciones matriciales.

Dada una imagen de salida en perspectiva de resolución $(W, H)$ y campo de visión $\text{FOV}$, primero establecemos un sistema de coordenadas de imagen normalizado:
$x = \frac{2u}{W-1} - 1, \quad y = \frac{2v}{H-1} - 1$
donde $(u, v)$ son coordenadas de píxel con $u \in [0, W-1]$ y $v \in [0, H-1]$.
La proyección en perspectiva mapea estas coordenadas 2D a rayos 3D sobre la esfera unitaria. La distancia focal en coordenadas normalizadas es:
$z = \frac{1}{\tan(\text{FOV}/2)}$.
Cada píxel $(x, y)$ corresponde a un vector de dirección 3D:
$\mathbf{d} = (x, y, z)$
Este vector se normaliza para situarse en la esfera unitaria:
$\mathbf{d}_{\text{norm}} = \frac{\mathbf{d}}{||\mathbf{d}||} = \frac{(x, y, z)}{\sqrt{x^2 + y^2 + z^2}}$

La dirección de visualización se controla mediante rotaciones de pitch ($\phi$) y yaw ($\theta$). Estas se aplican usando matrices de rotación 3D:

Rotación de pitch (rotación alrededor del eje Y):

$$R_{\text{yaw}}(\phi) = \begin{pmatrix}
\cos(\phi) & 0 & \sin(\phi) \\
0 & 1 & 0 \\
-\sin(\phi) & 0 & \cos(\phi)
\end{pmatrix}$$

\\
Rotación de yaw (rotación alrededor del eje X):

$$R_{\text{pitch}}(\theta) = \begin{pmatrix}
1 & 0 & 0 \\
0 & \cos(\theta) & -\sin(\theta) \\
0 & \sin(\theta) & \cos(\theta)
\end{pmatrix}$$

La matriz de rotación combinada es:
$R = R_{\text{yaw}}(\phi) \cdot R_{\text{pitch}}(\theta)$

Por lo tanto, el vector de dirección rotado se convierte en:
$\mathbf{d}_{\text{rot}} = R \cdot \mathbf{d}_{\text{norm}}$


El vector de dirección 3D rotado $(x', y', z')$ se convierte a coordenadas esféricas, calculando la longitud (ángulo azimutal) y la latitud (ángulo polar):


$$
\begin{align}
\phi &= \arctan2(x', z')\\
\lambda &= \arcsin(y')
\end{align}
$$
Donde $\phi \in [-\pi, \pi]$ y $\lambda \in [-\pi/2, \pi/2]$

Finalmente, las coordenadas esféricas se mapean a coordenadas de píxel en la imagen equirectangular de dimensiones $(W_{\text{equi}}, H_{\text{equi}})$:

Coordenada horizontal:
$u_{\text{equi}} = \left(\frac{\phi}{\pi} + 1\right) \cdot \frac{W_{\text{equi}}}{2}$

Coordenada vertical:
$v_{\text{equi}} = \left(\frac{1}{2} - \frac{\lambda}{\pi}\right) \cdot H_{\text{equi}}$

La transformación completa puede expresarse como la composición:
$\text{Perspectiva}(u,v) \rightarrow \text{Normalizada}(x,y,z) \rightarrow \text{Esfera}(x',y',z') \rightarrow \text{Esférica}(\phi,\lambda) \rightarrow \text{Equirectangular}(u_{\text{equi}},v_{\text{equi}})$


\subsection{Transformación de imágenes equirectangulares a Cubemap}

Se definen las 6 perspectivas principales, en grados, con la tabla \ref{tablaVistas}

\begin{table}{tablaVistas}{Equivalencia entre vistas de Cubemap y su Yaw y Pitch correspondiente}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Vista} & \textbf{Yaw} & \textbf{Pitch} & \\
		\midrule
		Lateral derecha & 90° & 0° & \\
		Trasera & 180° & 0° & \\
		Lateral izquierda & -90° & 0° & \\
		Frontal & 0° & 0° & \\
		Inferior & 0° & -90° & \\
		Superior & 0° & 90° & \\
		\bottomrule
	\end{tabular}
\end{table}

Usando todo el proceso descrito para extraer vistas de una imagen equirectangular, generamos para cada frame las 6 perspectivas principales para posteriormente procesarlas con un modelo de detección de objetos.

Se puede intuir que este proceso es muy costoso por lo que una optimización evidente será precomputar las tablas de busqueda $(u_f, v_f)$, permitiendo un remapeo eficiente en tiempo real usando la función \verb|cv2.remap()| de OpenCV con interpolación bilineal y condiciones de contorno cíclicas.

En el extracto de código \ref{precomputeMappingTables} se muestra la implementación del precomputo de las tables de mapeo usando el algoritmo descrito en el apartado anterior.

\PythonCode[precomputeMappingTables]{precomputeMappingTables}{Precomputación de tablas para un mapeo mas eficiente de las vistas de CubeMap}{EquirectProcessor.py}{29}{71}{}

\subsection{Detección de imágenes usando YOLO}
Una vez calculadas todas las perspectivas, se procede a la detección de imágenes usando el modelo de YOLO v8. Ultralytics ofrece una serie de tamaños dentro de cada versión de YOLO. Por limitaciones computacionales, se ha elegido el tamaño nano, el cual ofrece los peores resultados a cambio de los mejores tiempos. Esta es una decisión adecuada pues el trabajo esta orientada al deportista recreacional que no dispone de computación de altas prestaciones.

La librería de ultralytics facilita una clase que permite cargar un modelo, en este caso \verb|yolov8n.pt|, con toda la funcionalidad necesaria para procesar un frame y obtener una serie de resultados describiendo las detecciones encontradas. Por lo tanto, no se entra en mucho detalle de implementación en esta sección.

Sin embargo, si merece la pena mencionar el algoritmo empleado de estimación de distancia de objetos el cual a pesar de no ser completamente fiable, si dio buenos resultados en la práctica. En la figura \ref{objectDistance} se muestra el codigo que implementa la estimación de la distancia de un objeto usando el modelo de cámara estenopeica, el cual se basa en la longitud focal

\PythonCode[objectDistance]{objectDistance}{Implementación de la distancia de objetos basada en cámaras estenopeicas}{object-distance.py}{0}{37}{}

Esta distancia calculada se usará más adelante como variable dentro del algoritmo de detección y rastreo del portero.

Por último, cabe destacar que las dos únicas clases tenidas en cuenta dentro de las 80 clases detectadas por YOLO son la número 0 y la 32, las cuales hacen referencia a personas y pelotas de deportes respectivamente.

\subsection{Algoritmo de rastreo de portero}
Finalmente, con toda la información obtenida hasta ahora de las distintas perspectivas, se desarrolla el algoritmo de rastreo del portero, usando suposiciones básicas como la de que el portero probablemente sea el jugador más cercano a la porteria la mayor parte del tiempo, técnicas de suavizado de vídeo como la interpolación y triangulación entre la posición de la pelota y el portero para mostrar mejor información en el resultado final.

En primer lugar, el algoritmo separa las detecciones de personas y pelotas obtenidas. Normalmente solo se va a detectar una pelota pero puede suceder que el balón se encuentre entre dos perspectivas de la proyección de cubeMap o se de un falso positivo debido al tamaño del modelo de YOLO usado. 

Acto seguido, realiza una selección simple usando un sistema de pesos basado en la confianza y la distancia del objeto detectado, con un sesgo de 0.9 a favor de la distancia. Este es un ejemplo del tipo de normas y situaciones espécificas codificadas que permiten mejores resultados particularizando en un deporte.

Luego, se calcula el yaw y pitch base que apuntan directamente al portero. Estos primeros pasos asi como la cabecera del algoritmo vienen dados en el segmento de código \ref{Algorithm}

\PythonCode[Algorithm]{Algorithm}{Detección ingénua del portero en base a los resultados de YOLO}{Algorithm.py}{0}{35}{}

Con estos pasos realizados, se tiene ya una buena aproximación del frame que se debe generar para mostrar en el video final la jugada. Sin embargo, no se tiene en cuenta el posible efecto flickering causado por un falso positivo de un jugador que no sea el portero acercandose a la cámara ni se tiene en cuenta la posición del balón para mostrar, por ejemplo, el inicio de un tiro.

Por lo tanto, el siguiente paso será incorporar la información del balón recogida en la fase de detección si es que está disponible y realizar un sesgo del ángulo en favor del balón. Se ha decidido aplicar un sistema de pesos en función de la distancia del balón con condiciones que modifican de forma dínamica el peso de las coordenadas de la pelota.


\PythonCode[Algorithm2]{Algorithm2}{Incorporación de la bola en el algoritmo de rastreo}{Algorithm.py}{37}{61}{}

Finalmente, para resolver el problema del flickering previamente mencionado. Este problema se da en el caso de este trabajo cuando hay una diferencia considerable entre la predicción del algoritmo del frame $n$ y el frame previo $n-1$. El efecto resultante es una falta de cohesión en el video y en función del número de frames puede llegar a darse un parpadeo rápido que afecta gravemente a la calidad del video resultante. 

Se ha decidido aplicar técnicas básicas de suavizado de vídeo para prevenir el flickering. Primero, el algoritmo se asegura de que el salto de un frame a otro no es mayor que el umbral definido por argumentos. Luego, el algoritmo aplica una media movil exponencial (EMA) para suavizar el yaw y pitch previamente limitados.

\PythonCode[Algorithm3]{Algorithm3}{Aplicación de técnicas de suavizado de vídeo}{Algorithm.py}{63}{92}{}
