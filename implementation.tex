En este capítulo se entra en detalle sobre la implementación de los distintos componentes del sistema asi como las dificultades encontradas, la matemática detrás de las transformaciones y la complejidad computacional del algoritmo. Finalmente, se reflexiona sobre las decisiones tomadas y la correctitud de las mismas.

\section{Conversor de vídeos (media\_conversor)}

En primer lugar, se debe implementar el programa que convertirá los videos del formato propietario de insta360 a un formato público manteniendo las propiedades esféricas del vídeo.

Antes de desarrollar, se debe de tener acceso al SDK de insta360, al cual se aplica con razones acádemicas en este caso. El proceso de aprobación se extendió una semana y una vez obtenido, la implementación del conversor de videos fue muy sencilla pues el ejemplo de uso propuesto por la compañia es muy similar a la funcionalidad requerida para el proyecto. No se puede entrar muy en detalle de la implementación pues la mayor parte del trabajo la realiza la clase \verb|VideoStitcher| del SDK privado de insta360. Sin embargo, cabe mencionar que el programa viene con soporte de optimizaciones de GPU y paralelización de hilos.

El único desarrollo destacable es el proceso de elección de videos dínamico por el usuario pues a traves de una aplicación de CLI puesto que al tener dos lentes y un video asociado a cada lente, es poco práctico tener que escribir manualmente los \textit{paths} de entrada y salida del programa. A través del uso de Regex.

\CPPCode[picking_files_cpp]{File picking}{Automatización de I/O según la convención de insta360}{main.cc}{52}{86}{}

\vspace{60px}

\section{Rastreador del portero (goalkeeper\_tracker)}

Una vez obtenido el video en un formato con el que se puede trabajar con la librería de openCV, se procede a la implementación del principal objetivo del trabajo, la automatización de edición de jugadas de portero de futbol.

Los pasos a seguir por el programa, independientemente de su implementación son los siguientes.
\begin{enumerate}
	\item Elegir el vídeo de input en formato equirectangular y ya convertido a .mp4
	\item Iterar por cada frame que compone el video
	\item Convertir el frame equirectangular en su proyección de cubemap (6 perspectivas)
	\item Pasar cada una de las 6 perspectivas por el modelo de detección de objetos y  guardar los resultados de interes en una lista
	\item Usar la información de la lista para determinar la nueva perspectiva con la información relevante
	\item Crear la nueva perspectiva y escribirla en el vídeo resultante

\end{enumerate}

\subsection{Transformación de imágenes equirectangulares a Cubemap}

Se definen las 6 perspectivas principales, en grados, con la tabla \ref{tablaVistas}

\begin{table}{tablaVistas}{Equivalencia entre vistas de Cubemap y su Yaw y Pitch correspondiente}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Vista} & \textbf{Yaw} & \textbf{Pitch} & \\
		\midrule
		Lateral derecha & 90° & 0° & \\
		Trasera & 180° & 0° & \\
		Lateral izquierda & -90° & 0° & \\
		Frontal & 0° & 0° & \\
		Inferior & 0° & -90° & \\
		Superior & 0° & 90° & \\
		\bottomrule
	\end{tabular}
\end{table}

Usando todo el proceso descrito para extraer vistas de una imagen equirectangular, generamos para cada frame las 6 perspectivas principales para posteriormente procesarlas con un modelo de detección de objetos.

Se puede intuir que este proceso es muy costoso por lo que una optimización evidente será precomputar las tablas de busqueda $(u_f, v_f)$, permitiendo un remapeo eficiente en tiempo real usando la función \verb|cv2.remap()| de OpenCV con interpolación bilineal y condiciones de contorno cíclicas.

En el extracto de código \ref{precomputeMappingTables} se muestra la implementación del precomputo de las tables de mapeo usando el algoritmo descrito en el apartado anterior.

\PythonCode[precomputeMappingTables]{precomputeMappingTables}{Precomputación de tablas para un mapeo mas eficiente de las vistas de CubeMap}{EquirectProcessor.py}{29}{71}{}

\subsection{Detección de imágenes usando YOLO}
Una vez calculadas todas las perspectivas, se procede a la detección de imágenes usando el modelo de YOLO v8. Ultralytics ofrece una serie de tamaños dentro de cada versión de YOLO. Por limitaciones computacionales, se ha elegido el tamaño nano, el cual ofrece los peores resultados a cambio de los mejores tiempos. Esta es una decisión adecuada pues el trabajo esta orientada al deportista recreacional que no dispone de computación de altas prestaciones.

La librería de ultralytics facilita una clase que permite cargar un modelo, en este caso \verb|yolov8n.pt|, con toda la funcionalidad necesaria para procesar un frame y obtener una serie de resultados describiendo las detecciones encontradas. Por lo tanto, no se entra en mucho detalle de implementación en esta sección.

Sin embargo, si merece la pena mencionar el algoritmo empleado de estimación de distancia de objetos el cual a pesar de no ser completamente fiable, si dio buenos resultados en la práctica. En la figura \ref{objectDistance} se muestra el codigo que implementa la estimación de la distancia de un objeto usando el modelo de cámara estenopeica, el cual se basa en la longitud focal

\PythonCode[objectDistance]{objectDistance}{Implementación de la distancia de objetos basada en cámaras estenopeicas}{object-distance.py}{0}{37}{}

Esta distancia calculada se usará más adelante como variable dentro del algoritmo de detección y rastreo del portero.

Por último, cabe destacar que las dos únicas clases tenidas en cuenta dentro de las 80 clases detectadas por YOLO son la número 0 y la 32, las cuales hacen referencia a personas y pelotas de deportes respectivamente.

\subsection{Algoritmo de rastreo de portero}
Finalmente, con toda la información obtenida hasta ahora de las distintas perspectivas, se desarrolla el algoritmo de rastreo del portero, usando suposiciones básicas como la de que el portero probablemente sea el jugador más cercano a la porteria la mayor parte del tiempo, técnicas de suavizado de vídeo como la interpolación y triangulación entre la posición de la pelota y el portero para mostrar mejor información en el resultado final.

En primer lugar, el algoritmo separa las detecciones de personas y pelotas obtenidas. Normalmente solo se va a detectar una pelota pero puede suceder que el balón se encuentre entre dos perspectivas de la proyección de cubeMap o se de un falso positivo debido al tamaño del modelo de YOLO usado. 

Acto seguido, realiza una selección simple usando un sistema de pesos basado en la confianza y la distancia del objeto detectado, con un sesgo de 0.9 a favor de la distancia. Este es un ejemplo del tipo de normas y situaciones espécificas codificadas que permiten mejores resultados particularizando en un deporte.

Luego, se calcula el yaw y pitch base que apuntan directamente al portero. Estos primeros pasos asi como la cabecera del algoritmo vienen dados en el segmento de código \ref{Algorithm}

\PythonCode[Algorithm]{Algorithm}{Detección ingénua del portero en base a los resultados de YOLO}{Algorithm.py}{0}{35}{}

Con estos pasos realizados, se tiene ya una buena aproximación del frame que se debe generar para mostrar en el video final la jugada. Sin embargo, no se tiene en cuenta el posible efecto flickering causado por un falso positivo de un jugador que no sea el portero acercandose a la cámara ni se tiene en cuenta la posición del balón para mostrar, por ejemplo, el inicio de un tiro.

Por lo tanto, el siguiente paso será incorporar la información del balón recogida en la fase de detección si es que está disponible y realizar un sesgo del ángulo en favor del balón. Se ha decidido aplicar un sistema de pesos en función de la distancia del balón con condiciones que modifican de forma dínamica el peso de las coordenadas de la pelota.


\PythonCode[Algorithm2]{Algorithm2}{Incorporación de la bola en el algoritmo de rastreo}{Algorithm.py}{37}{61}{}

Finalmente, para resolver el problema del flickering previamente mencionado. Este problema se da en el caso de este trabajo cuando hay una diferencia considerable entre la predicción del algoritmo del frame $n$ y el frame previo $n-1$. El efecto resultante es una falta de cohesión en el video y en función del número de frames puede llegar a darse un parpadeo rápido que afecta gravemente a la calidad del video resultante. 

Se ha decidido aplicar técnicas básicas de suavizado de vídeo para prevenir el flickering. Primero, el algoritmo se asegura de que el salto de un frame a otro no es mayor que el umbral definido por argumentos. Luego, el algoritmo aplica una media movil exponencial (EMA) para suavizar el yaw y pitch previamente limitados.

\PythonCode[Algorithm3]{Algorithm3}{Aplicación de técnicas de suavizado de vídeo}{Algorithm.py}{63}{92}{}
