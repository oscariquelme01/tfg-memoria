\section[Decisiones de diseño]{Decisiones de diseño y tecnologías}
Antes de comenzar con la implementación del proyecto, se deben tomar una serie de decisiones de diseño sobre las distintas posibilidades estudiadas en el capítulo de Estado del Arte.

\subsection{Dispositivo de grabación}

Para el caso de estudio de este trabajo, se ha optado por una cámara de 360 grados pues, aunque el resultado esperado es una sucesión de imágenes en perspectiva sin distorsión, este tipo de dispositivos permiten capturar toda la información alrededor del portero que más adelante permitirá tomar decisiones automáticas sobre qué perspectiva debemos extraer para cada imagen.

En particular, se trabajará con la cámara ONE X2 de la compañía Insta360. Esta es capaz de grabar a 60FPS en 5.7k a 360 grados y dispone de un SDK privado al que podemos acceder con fines académicos y comerciales. Es necesario utilizar dicho SDK puesto que la cámara graba en un formato propietario llamado \textit{.insv}. Para manejar los vídeos grabados utilizando librerías públicas, primero deberemos realizar la conversión a un formato público usando la librería de Insta360.

También se ha adquirido un trípode para nivelar la cámara a la altura deseada y facilitar el trabajo de detección de imágenes a los modelos de visión computacional.

Esta decisión viene con una serie de desventajas las cuales se resumen en un incremento notorio de capacidad computacional requerida, una mayor complejidad a la hora de trabajar con el sistema de coordenadas equirectangular y una serie de desafíos técnicos que se detallarán en el capítulo de implementación.

\subsection{Modelo de detección de objetos}

Dentro de los modelos contemplados durante el estado del arte, YOLO resulta ser la elección más apropiada por varias razones específicas. Primero, el portero representa típicamente un objeto de tamaño considerable en el campo de visión, minimizando la desventaja histórica de YOLO con objetos pequeños. Segundo, las cámaras de 360 grados generan grandes volúmenes de datos que necesitan procesamiento eficiente, y la arquitectura más simple de YOLO consume menos recursos computacionales que el enfoque multiescala de SSD, reduciendo significativamente los tiempos de procesamiento del vídeo. Tercero, para aplicaciones de tracking de un objeto específico como el portero, la consistencia en las detecciones y la eficiencia computacional son más importantes que la precisión absoluta de detección multiclase.

\subsection{Tecnologías y librerías}

La decisión de qué tecnologías y librerías escoger ha sido sencilla. La mayor parte del proyecto se desarrolla en Python, dejando solo la parte de conversión de vídeo de formato propietario a público en C++ porque el SDK de Insta360 lo requiere.

Las principales librerías de interés de Python usadas son las siguientes:
\begin{description}
	\item[ultralytics:] librería oficial de la compañía Ultralytics para cargar y usar modelos como el de YOLO.
	\item[numpy:] librería de funciones matemáticas para el trabajo de arrays multidimensionales, pues está escrita en C y optimizada para un volumen de cálculos matemáticos muy grande. Es el estándar de la industria y es de código abierto.
	\item[cv2:] principal módulo de Python de la librería de visión computacional más grande del mundo, OpenCV, con más de 2500 algoritmos y operada por la organización sin ánimo de lucro Open Source Vision Foundation.
\end{description}


\section[Arquitectura]{Arquitectura del sistema}
El sistema se compone de dos actores principales encargados de leer y escribir los vídeos grabados por la cámara e importados de forma manual. El proceso de importación de vídeo puede ser automatizado también, pero se ha decidido que está fuera de alcance del proyecto y puede ser incluido como trabajo futuro para minimizar aún más el tiempo invertido por el deportista.
\vspace{20px}

\subsection{Conversor de vídeos (media\_conversor)}
El primer actor del sistema será el conversor de vídeos de formato propietario a público. Se implementa en C++ porque el propio SDK de Insta360 está implementado en esta tecnología. El programa se limita a tomar como input una pareja de vídeos guardada en \verb|sources/raw_footage| en formato \textit{.insv} debido a que la cámara graba en dos ficheros distintos cada vídeo, uno por cada lente. Una vez seleccionado un vídeo, procederá a realizar las transformaciones pertinentes para guardar el vídeo en formato \textit{.mp4} en la carpeta de \verb|sources/converted_footage|.

\subsection{Rastreador del portero (goalkeeper\_tracker)}
El segundo actor es de mucho mayor interés y encapsula toda la funcionalidad de identificar al portero en un partido y asegurar que la imagen equirectangular se transforme en la perspectiva adecuada que muestra la información relevante de la jugada. Dentro de este actor se desarrollan distintos módulos como el de estimación de distancias de objetos detectados, el procesador de imágenes equirectangulares y el traductor de coordenadas entre esféricas, equirectangulares y cartesianas.


\begin{figure}[Arquitectura del sistema]{design}{Diagrama de la arquitectura del sistema}
	\begin{center}
		\image{}{}{assets/design}
	\end{center}
\end{figure}

\section{Diseño del algoritmo}
La parte más crítica del pipeline es el algoritmo que se emplea para decidir qué perspectiva debemos producir a partir de una imagen equirectangular.

Como se ha discutido previamente, no es buena idea realizar la detección de imágenes sobre la propia imagen equirectangular usando un modelo entrenado con imágenes planas, por lo que el primer paso del algoritmo será transformar la imagen equirectangular en su mapa cúbico (cubeMap), lo cual significa obtener las 6 perspectivas planas (frontal, laterales, trasera...) que describen la información contenida en la imagen equirectangular eliminando la distorsión completamente.

El segundo paso es procesar cada una de las perspectivas generadas a través del modelo de detección de visión computacional de YOLO. Esto genera una lista de objetos detectados, entre los cuales se filtra según nuestras clases de interés. Una vez obtenida una lista curada de detecciones, se calcula la distancia estimada a cada objeto almacenando toda la información de interés hasta el momento.

Acto seguido, se iterará sobre la lista obtenida, seleccionando a la persona más cercana a la cámara y triangulando entre el balón (en caso de haber sido detectado) y el portero o simplemente ajustando la vista al portero.

Finalmente, se realizan las traducciones necesarias de las coordenadas cartesianas obtenidas a las coordenadas equirectangulares y se crea la nueva perspectiva que describe una vista adecuada para la jugada, escribiéndola en el vídeo resultante.
